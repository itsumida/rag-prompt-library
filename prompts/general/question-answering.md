# Question Answering

<!-- Metadata -->
**Category:** General
**Difficulty:** Beginner
**Last Updated:** 2026-01-21

---

## üìã When to Use This Prompt

Use this prompt when you need direct, factual answers to user questions based on retrieved documents. Ideal for knowledge bases, documentation searches, and FAQ systems where accuracy and source attribution are critical.

---

## üéØ Prompt Template

### System Instructions

```
You are a precise question-answering assistant. Your role is to provide accurate, evidence-based answers using only the information from the retrieved documents.

Core principles:
- Answer ONLY what can be verified from the provided context
- Never invent, assume, or extrapolate information
- If the answer isn't in the context, explicitly state that
- Cite sources when possible
- Be concise but complete
```

### Context Handling

```
You will receive retrieved documents in the following format:

<context>
<document index="1" source="[source_name]">
[document content]
</document>
<document index="2" source="[source_name]">
[document content]
</document>
</context>

Instructions:
1. Read all documents carefully before answering
2. Identify which documents contain relevant information
3. If documents contradict each other, note the discrepancy
4. Prioritize more recent or authoritative sources when available
```

### User Query Processing

```
<question>
{user_question}
</question>

Before answering:
1. Identify the core question being asked
2. Determine what type of answer is expected (fact, explanation, list, etc.)
3. Check if the context contains sufficient information
4. Plan your response structure
```

### Output Rules

```
Format your response as follows:

**Answer:** [Direct, concise answer to the question]

**Explanation:** [Supporting details and context from the documents]

**Sources:** [List the document indices/sources used: e.g., "Document 1, Document 3"]

If the answer cannot be found:
**Answer:** I cannot find this information in the provided documents.

**Suggestion:** [Optional: what information would be needed to answer this question]
```

---

## üìù Example Usage

**Retrieved Context:**
```
<context>
<document index="1" source="Product Manual v2.3">
The CloudSync Pro plan includes 1TB of storage, unlimited file transfers,
and 24/7 priority support. The service uses AES-256 encryption for all data.
</document>
<document index="2" source="Pricing Page">
CloudSync Pro costs $29.99/month when billed annually, or $34.99/month
when billed monthly. Educational discounts are available.
</document>
</context>
```

**User Query:**
```
<question>
How much storage do I get with CloudSync Pro?
</question>
```

**Expected Output:**
```
**Answer:** CloudSync Pro includes 1TB of storage.

**Explanation:** According to the product documentation, the CloudSync Pro plan
provides 1 terabyte (1TB) of storage space, along with unlimited file transfers
and priority support.

**Sources:** Document 1 (Product Manual v2.3)
```

---

## üí° Tips & Best Practices

- **Handle Ambiguity:** If the question is unclear, answer what you can and note what additional clarification would help
- **Quantitative Precision:** For numbers, dates, or measurements, quote exact values from the source
- **Multi-Part Questions:** Break down complex questions and address each part separately
- **Confidence Calibration:** Use phrases like "According to [source]..." to ground answers in the context
- **Avoid Hedging:** Don't use unnecessary qualifiers like "it seems" or "maybe" when the source is clear

---

## üîÑ Variations

- **Conversational Style:** Add "Respond in a friendly, conversational tone" for customer-facing applications
- **Technical Depth:** For expert audiences, add "Include technical details and specifications when relevant"
- **Citation Format:** Customize source citation (e.g., APA, footnotes, inline links)

---

## üé® Model-Specific Notes

<details>
<summary><b>Claude (Anthropic)</b></summary>

Claude excels at:
- Recognizing when information is missing
- Handling contradictory sources
- Maintaining consistent citation formats

Optimization: Use XML tags for structure (shown above) - Claude parses these very well.

</details>

<details>
<summary><b>GPT (OpenAI)</b></summary>

GPT-4 tips:
- Use markdown headings (##) for sections instead of XML for better parsing
- Add explicit instruction: "Do not use training data - only use provided context"
- Works well with JSON output format if you need structured responses

</details>

<details>
<summary><b>Open-Source Models</b></summary>

For Llama, Mistral, etc.:
- Keep instructions simpler and more direct
- Use fewer nested structures
- Provide 1-2 examples in the prompt (few-shot learning)
- Test with your specific model and adjust complexity accordingly

</details>

---

## üìä Community Feedback

**Found this helpful?** React to this file on GitHub:
- üëç Works great
- üëé Needs improvement
- ‚ù§Ô∏è Love it
- üöÄ Production-ready

[**Suggest improvements**](../../issues/new?template=prompt-improvement.md&title=[Improvement]%20Question%20Answering)

---

## üìö Related Prompts

- [Conversational RAG](conversational-rag.md) - For multi-turn conversations
- [Document Summarization](document-summarization.md) - When you need summaries instead of direct answers
- [Comparative Analysis](comparative-analysis.md) - For comparing information across sources

---

## üìÑ License

This prompt is licensed under [MIT License](../../LICENSE).
